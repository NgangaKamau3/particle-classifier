{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electron-Photon Classifier\n",
    "\n",
    "\n",
    "\n",
    "This notebook implements a deep learning model for classifying electrons and photons using a ResNet-15 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import random\n",
    "import seaborn as sns\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Residual Block Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for particle data with memory-efficient loading\"\"\"\n",
    "    def __init__(self, electron_path, photon_path, transform=None, chunk_size=1000):\n",
    "        self.transform = transform\n",
    "        self.electron_path = electron_path\n",
    "        self.photon_path = photon_path\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # Get dataset sizes without loading full data\n",
    "        with h5py.File(electron_path, 'r') as f:\n",
    "            self.electron_len = len(f['X'])\n",
    "            \n",
    "        with h5py.File(photon_path, 'r') as f:\n",
    "            self.photon_len = len(f['X'])\n",
    "            \n",
    "        self.total_len = self.electron_len + self.photon_len\n",
    "        \n",
    "        # Create shuffled indices\n",
    "        self.indices = np.random.permutation(self.total_len)\n",
    "        \n",
    "        print(f\"Dataset initialized with {self.total_len} samples\")\n",
    "        print(f\"Electron samples: {self.electron_len}\")\n",
    "        print(f\"Photon samples: {self.photon_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map shuffled index to original index\n",
    "        orig_idx = self.indices[idx]\n",
    "        \n",
    "        # Determine which file to load from\n",
    "        if orig_idx < self.electron_len:\n",
    "            file_path = self.electron_path\n",
    "            local_idx = orig_idx\n",
    "            label = 0  # electron\n",
    "        else:\n",
    "            file_path = self.photon_path\n",
    "            local_idx = orig_idx - self.electron_len\n",
    "            label = 1  # photon\n",
    "        \n",
    "        # Load single sample from appropriate file\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            data = f['X'][local_idx]\n",
    "        \n",
    "        # Reshape if needed\n",
    "        if len(data.shape) == 2:  # If shape is (height, width)\n",
    "            data = data.reshape(2, -1)  # Reshape to (channels, features)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.FloatTensor(data)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Implementation of a single residual block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-15 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet15(nn.Module):\n",
    "    \"\"\"Flexible ResNet-15 architecture that can be scaled in width and depth\"\"\"\n",
    "    def __init__(self, num_classes=2, width_factor=1.0, depth_factor=1.0, dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes\n",
    "            width_factor (float): Factor to scale channel widths (1.0 = standard)\n",
    "            depth_factor (float): Factor to scale number of blocks (1.0 = standard)\n",
    "            dropout_rate (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(ResNet15, self).__init__()\n",
    "        \n",
    "        # Scale the base channel sizes\n",
    "        base_channels = int(32 * width_factor)\n",
    "        self.in_channels = base_channels\n",
    "        \n",
    "        # Initial convolution with scaled channels\n",
    "        self.conv1 = nn.Conv2d(2, base_channels, kernel_size=3, \n",
    "                              stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(base_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Calculate number of blocks per layer\n",
    "        blocks_per_layer = int(2 * depth_factor)\n",
    "        \n",
    "        # Residual blocks with scaled channels\n",
    "        self.layer1 = self._make_layer(base_channels, blocks_per_layer, stride=1)\n",
    "        self.layer2 = self._make_layer(base_channels * 2, blocks_per_layer, stride=2)\n",
    "        self.layer3 = self._make_layer(base_channels * 4, blocks_per_layer, stride=2)\n",
    "        \n",
    "        # Final layers\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(base_channels * 4, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        # First block handles stride and channel changes\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        \n",
    "        # Remaining blocks maintain dimensions\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model variants\n",
    "class ResNet15Wide(ResNet15):\n",
    "    \"\"\"Wider variant of ResNet15\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            width_factor=1.5,    # 50% wider channels\n",
    "            depth_factor=1.0,    # Same depth\n",
    "            dropout_rate=0.3\n",
    "        )\n",
    "\n",
    "class ResNet15Deep(ResNet15):\n",
    "    \"\"\"Deeper variant of ResNet15\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            width_factor=1.0,    # Same width\n",
    "            depth_factor=1.5,    # 50% more blocks\n",
    "            dropout_rate=0.3\n",
    "        )\n",
    "\n",
    "def get_model_variant(variant='standard', num_classes=2):\n",
    "    \"\"\"Get specific model variant\"\"\"\n",
    "    variants = {\n",
    "        'standard': lambda: ResNet15(num_classes=num_classes, width_factor=1.0, depth_factor=1.0),\n",
    "        'wide': lambda: ResNet15Wide(num_classes=num_classes),\n",
    "        'deep': lambda: ResNet15Deep(num_classes=num_classes)\n",
    "    }\n",
    "    return variants[variant]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, batch_size=128, num_epochs=30, device='cuda'):  # Increased batch size, reduced epochs\n",
    "    \"\"\"Memory-efficient and faster training function\"\"\"\n",
    "    \n",
    "    # Split dataset with reduced validation set\n",
    "    train_size = int(0.9 * len(dataset))  # Increased train size\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Optimize data loading\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Increased workers\n",
    "        pin_memory=True if device=='cuda' else False,\n",
    "        persistent_workers=True,  # Keep workers alive between epochs\n",
    "        prefetch_factor=2  # Prefetch next batches\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size*2,  # Larger batch size for validation\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if device=='cuda' else False,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # Use mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Optimize training components\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(  # Changed to AdamW\n",
    "        model.parameters(),\n",
    "        lr=0.002,  # Increased learning rate\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-7\n",
    "    )\n",
    "    \n",
    "    # More aggressive learning rate scheduling\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.002,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=10\n",
    "    )\n",
    "    \n",
    "    # Training loop with mixed precision\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Optimize with gradient scaling\n",
    "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step()  # Update LR each step\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if torch.cuda.is_available() and (total % (10 * batch_size) == 0):\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Quick validation\n",
    "        if epoch % 2 == 0:  # Validate every 2 epochs\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}: Train Acc: {100.*correct/total:.2f}%, Val Acc: {100.*val_correct/val_total:.2f}%\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Model evaluation function\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            \n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (photon)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Electron', 'Photon'], \n",
    "                yticklabels=['Electron', 'Photon'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'true_labels': true_labels,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Function to plot training metrics\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=5):\n",
    "    \"\"\"Function to visualize dataset samples\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 3*num_samples))\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        data, label = dataset[idx]\n",
    "        \n",
    "        # Energy channel\n",
    "        im0 = axes[i, 0].imshow(data[0], cmap='viridis')\n",
    "        axes[i, 0].set_title(f\"Sample {idx} - {'Photon' if label == 1 else 'Electron'} - Energy\")\n",
    "        axes[i, 0].axis('off')\n",
    "        fig.colorbar(im0, ax=axes[i, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Time channel\n",
    "        im1 = axes[i, 1].imshow(data[1], cmap='plasma')\n",
    "        axes[i, 1].set_title(f\"Sample {idx} - {'Photon' if label == 1 else 'Electron'} - Time\")\n",
    "        axes[i, 1].axis('off')\n",
    "        fig.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, dataset, layer_name, device='cuda'):\n",
    "    \"\"\"Function to visualize model feature maps\"\"\"\n",
    "    \"\"\"\n",
    "    Visualize feature maps from a specific layer of the model for a sample image\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Select a random sample\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    input_img, label = dataset[idx]\n",
    "    input_img = input_img.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Register hook to get intermediate activations\n",
    "    activations = {}\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        activations[layer_name] = output.detach().cpu()\n",
    "    \n",
    "    # Register hook\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name:\n",
    "            hook = module.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_img)\n",
    "    \n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "    \n",
    "    # Get the feature maps\n",
    "    feature_maps = activations[layer_name][0]  # First sample in batch\n",
    "    num_features = min(16, feature_maps.size(0))  # Display at most 16 feature maps\n",
    "    \n",
    "    # Plot the feature maps\n",
    "    fig, axes = plt.subplots(int(np.ceil(num_features/4)), 4, figsize=(12, 3*int(np.ceil(num_features/4))))\n",
    "    fig.suptitle(f\"Feature maps from {layer_name} for {'Photon' if label == 1 else 'Electron'}\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        ax = axes[i//4, i%4] if num_features > 4 else axes[i]\n",
    "        im = ax.imshow(feature_maps[i], cmap='viridis')\n",
    "        ax.set_title(f\"Filter {i}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide any empty subplots\n",
    "    for i in range(num_features, int(np.ceil(num_features/4))*4):\n",
    "        if i < int(np.ceil(num_features/4))*4:\n",
    "            axes[i//4, i%4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = os.path.join('datasets', 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5') \n",
    "    photon_path = os.path.join('datasets', 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5') \n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Visualize some samples\n",
    "    visualize_samples(full_dataset)\n",
    "    \n",
    "    # Split dataset into train (80%) and test (20%) sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Further split train set into train and validation\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    print(f\"Test set size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Create model\n",
    "    model = ResNet15(num_classes=2).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=50,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate model on test set\n",
    "    test_results = evaluate_model(trained_model, test_loader, device)\n",
    "    \n",
    "    # Visualize feature maps\n",
    "    visualize_feature_maps(trained_model, test_dataset, 'layer3.1.conv2', device)\n",
    "    \n",
    "    # Save model weights\n",
    "    torch.save(trained_model.state_dict(), 'resnet15_electron_photon_classifier.pth')\n",
    "    print(\"Model saved to resnet15_electron_photon_classifier.pth\")\n",
    "    \n",
    "    # Save test results\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_labels': test_results['true_labels'],\n",
    "        'predictions': test_results['predictions'],\n",
    "        'probabilities': test_results['probabilities']\n",
    "    })\n",
    "    results_df.to_csv('test_results.csv', index=False)\n",
    "    print(\"Test results saved to test_results.csv\")\n",
    "    \n",
    "    return trained_model, test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_experiment():\n",
    "    \"\"\"Data augmentation experiment\"\"\"\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = 'datasets/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
    "    photon_path = 'datasets/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
    "\n",
    "    # Basic dataset without augmentation\n",
    "    basic_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Define augmentation transforms\n",
    "    class RandomNoise(object):\n",
    "        def __init__(self, noise_level=0.05):\n",
    "            self.noise_level = noise_level\n",
    "        \n",
    "        def __call__(self, sample):\n",
    "            noise = np.random.normal(0, self.noise_level, sample.shape)\n",
    "            noisy_sample = sample + noise\n",
    "            return noisy_sample\n",
    "    \n",
    "    class RandomFlip(object):\n",
    "        def __call__(self, sample):\n",
    "            if np.random.random() > 0.5:\n",
    "                return np.flip(sample, axis=1)  # Flip horizontally\n",
    "            return sample\n",
    "    \n",
    "    class RandomRotate90(object):\n",
    "        def __call__(self, sample):\n",
    "            k = np.random.randint(0, 4)  # 0, 1, 2, or 3 times 90 degrees\n",
    "            return np.rot90(sample, k=k, axes=(1, 2))\n",
    "    \n",
    "    # Create augmented dataset\n",
    "    class AugmentedParticleDataset(ParticleDataset):\n",
    "        def __init__(self, electron_path, photon_path):\n",
    "            super().__init__(electron_path, photon_path)\n",
    "            self.transform = transforms.Compose([\n",
    "                RandomNoise(),\n",
    "                RandomFlip(),\n",
    "                RandomRotate90()\n",
    "            ])\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "            \n",
    "            sample = self.data[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # Apply data augmentation\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "            \n",
    "            # Convert to torch tensors\n",
    "            sample = torch.from_numpy(sample).float()\n",
    "            label = torch.tensor(label).long()\n",
    "            \n",
    "            return sample, label\n",
    "    \n",
    "    # Create augmented dataset\n",
    "    augmented_dataset = AugmentedParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Split datasets\n",
    "    train_size = int(0.8 * len(basic_dataset))\n",
    "    test_size = len(basic_dataset) - train_size\n",
    "    basic_train, basic_test = random_split(basic_dataset, [train_size, test_size])\n",
    "    augmented_train, augmented_test = random_split(augmented_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = 64\n",
    "    basic_train_loader = DataLoader(basic_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    basic_val_loader = DataLoader(basic_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    augmented_train_loader = DataLoader(augmented_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    augmented_val_loader = DataLoader(augmented_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Train model without augmentation\n",
    "    basic_model = ResNet15(num_classes=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(basic_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    basic_model, basic_history = train_model(\n",
    "        model=basic_model,\n",
    "        train_loader=basic_train_loader,\n",
    "        val_loader=basic_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=30,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train model with augmentation\n",
    "    augmented_model = ResNet15(num_classes=2).to(device)\n",
    "    optimizer = optim.Adam(augmented_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    augmented_model, augmented_history = train_model(\n",
    "        model=augmented_model,\n",
    "        train_loader=augmented_train_loader,\n",
    "        val_loader=augmented_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=30,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Compare results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(basic_history['val_loss'], label='Without Augmentation')\n",
    "    plt.plot(augmented_history['val_loss'], label='With Augmentation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Validation Loss Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(basic_history['val_acc'], label='Without Augmentation')\n",
    "    plt.plot(augmented_history['val_acc'], label='With Augmentation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Validation Accuracy Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Final validation accuracy without augmentation:\", basic_history['val_acc'][-1])\n",
    "    print(\"Final validation accuracy with augmentation:\", augmented_history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning():\n",
    "    \"\"\"Hyperparameter tuning experiment\"\"\"\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = 'datasets/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
    "    photon_path = 'datasets/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Split dataset into train (80%) and test (20%) sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Further split train set into train and validation\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Define hyperparameter combinations\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    weight_decays = [1e-4, 1e-5, 1e-6]\n",
    "    dropout_rates = [0.2, 0.3, 0.4]\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Grid search\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decays:\n",
    "            for dr in dropout_rates:\n",
    "                print(f\"Training with lr={lr}, weight_decay={wd}, dropout={dr}\")\n",
    "                \n",
    "                # Create model with specified dropout rate\n",
    "                class ResNet15WithDropout(ResNet15):\n",
    "                    def __init__(self, num_classes=2, dropout_rate=0.3):\n",
    "                        super().__init__(num_classes)\n",
    "                        self.dropout = nn.Dropout(dropout_rate)\n",
    "                \n",
    "                model = ResNet15WithDropout(num_classes=2, dropout_rate=dr).to(device)\n",
    "                \n",
    "                # Define loss function and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "                \n",
    "                # Train model for fewer epochs for quicker tuning\n",
    "                trained_model, history = train_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    num_epochs=20,  # Reduced epochs for quicker tuning\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                trained_model.eval()\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = trained_model(inputs)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                val_accuracy = val_correct / val_total\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'weight_decay': wd,\n",
    "                    'dropout_rate': dr,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'final_val_loss': history['val_loss'][-1]\n",
    "                })\n",
    "    \n",
    "    # Convert results to DataFrame and find best hyperparameters\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_config = results_df.loc[results_df['val_accuracy'].idxmax()]\n",
    "    \n",
    "    print(\"\\nHyperparameter Tuning Results:\")\n",
    "    print(results_df.sort_values('val_accuracy', ascending=False))\n",
    "    print(\"\\nBest Configuration:\")\n",
    "    print(best_config)\n",
    "    \n",
    "    return best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_model():\n",
    "    \"\"\"Create and train ensemble of model variants\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create models\n",
    "    models = {\n",
    "        'standard': get_model_variant('standard').to(device),\n",
    "        'wide': get_model_variant('wide').to(device),\n",
    "        'deep': get_model_variant('deep').to(device)\n",
    "    }\n",
    "    \n",
    "    # Create dataset\n",
    "    electron_path = os.path.join('datasets', 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "    photon_path = os.path.join('datasets', 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "    dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Train each model\n",
    "    trained_models = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        trained_model = train_model(\n",
    "            model=model,\n",
    "            dataset=dataset,\n",
    "            batch_size=128,\n",
    "            num_epochs=30,\n",
    "            device=device\n",
    "        )\n",
    "        trained_models[name] = trained_model\n",
    "    \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, input_batch):\n",
    "    \"\"\"Make predictions using ensemble of models\"\"\"\n",
    "    predictions = []\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_batch)\n",
    "            predictions.append(torch.softmax(outputs, dim=1))\n",
    "    return torch.stack(predictions).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Electron file structure:\n",
      "Available datasets: ['X', 'y']\n",
      "X shape: (249000, 32, 32, 2)\n",
      "X dtype: float32\n",
      "y shape: (249000,)\n",
      "y dtype: float32\n",
      "\n",
      "Photon file structure:\n",
      "Available datasets: ['X', 'y']\n",
      "X shape: (249000, 32, 32, 2)\n",
      "X dtype: float32\n",
      "y shape: (249000,)\n",
      "y dtype: float32\n",
      "\n",
      "=== Main Training Pipeline ===\n",
      "Dataset initialized with 498000 samples\n",
      "Electron samples: 249000\n",
      "Photon samples: 249000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_13912\\1767083690.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Epoch 1/50:   0%|          | 0/7004 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def check_data_paths():\n",
    "    \"\"\"Check if data files exist and are accessible, and verify their structure\"\"\"\n",
    "    # Use os.path.join for cross-platform compatibility\n",
    "    data_dir = os.path.join('datasets')\n",
    "    electron_path = os.path.join(data_dir, 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "    photon_path = os.path.join(data_dir, 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "    \n",
    "    # Create datasets directory if it doesn't exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"Created directory: {data_dir}\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    files_exist = True\n",
    "    if not os.path.isfile(electron_path):\n",
    "        print(f\"Missing electron file: {electron_path}\")\n",
    "        files_exist = False\n",
    "    if not os.path.isfile(photon_path):\n",
    "        print(f\"Missing photon file: {photon_path}\")\n",
    "        files_exist = False\n",
    "    \n",
    "    if not files_exist:\n",
    "        print(\"\\nPlease download the dataset files and place them in the 'datasets' directory.\")\n",
    "        return False\n",
    "    \n",
    "    # Check file structure\n",
    "    try:\n",
    "        # Check electron file\n",
    "        with h5py.File(electron_path, 'r') as f:\n",
    "            print(\"\\nElectron file structure:\")\n",
    "            keys = list(f.keys())\n",
    "            print(\"Available datasets:\", keys)\n",
    "            \n",
    "            # Print detailed information about each dataset\n",
    "            for key in keys:\n",
    "                print(f\"{key} shape:\", f[key].shape)\n",
    "                print(f\"{key} dtype:\", f[key].dtype)\n",
    "    \n",
    "        # Check photon file\n",
    "        with h5py.File(photon_path, 'r') as f:\n",
    "            print(\"\\nPhoton file structure:\")\n",
    "            keys = list(f.keys())\n",
    "            print(\"Available datasets:\", keys)\n",
    "            \n",
    "            # Print detailed information about each dataset\n",
    "            for key in keys:\n",
    "                print(f\"{key} shape:\", f[key].shape)\n",
    "                print(f\"{key} dtype:\", f[key].dtype)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except OSError as e:\n",
    "        print(f\"\\nError reading HDF5 files: {str(e)}\")\n",
    "        print(\"Please ensure the files are valid HDF5 format.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error: {str(e)}\")\n",
    "        print(\"Please check the file permissions and format.\")\n",
    "        return False\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if data files exist and have correct structure\n",
    "    if check_data_paths():\n",
    "        # Main execution\n",
    "        print(\"\\n=== Main Training Pipeline ===\")\n",
    "        # Create dataset and prepare for training\n",
    "        electron_path = os.path.join('datasets', 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "        photon_path = os.path.join('datasets', 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "        full_dataset = ParticleDataset(electron_path, photon_path)\n",
    "\n",
    "        # Train the model using the full dataset\n",
    "        model = ResNet15(num_classes=2)\n",
    "        trained_model, history = train_model(\n",
    "            model=model,\n",
    "            dataset=full_dataset,\n",
    "            batch_size=64,\n",
    "            num_epochs=50,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Data Augmentation Experiment ===\")\n",
    "        augmentation_experiment()\n",
    "        \n",
    "        print(\"\\n=== Hyperparameter Tuning ===\")\n",
    "        best_hyperparams = hyperparameter_tuning()\n",
    "        \n",
    "        print(\"\\n=== Model Ensemble ===\")\n",
    "        ensemble_results = create_ensemble_model()\n",
    "    else:\n",
    "        print(\"\\nExecution stopped due to missing or invalid data files.\")\n",
    "        print(\"Please check the file structure and ensure it contains 'energy' and 'time' datasets.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
