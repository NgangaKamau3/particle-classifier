{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Particle Classification Model\n",
    "\n",
    "\n",
    "\n",
    " This notebook implements a ResNet-15 architecture for classifying electron and photon particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n",
    "import random\n",
    "import seaborn as sns\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for particle data with memory-efficient loading\"\"\"\n",
    "    def __init__(self, electron_path, photon_path, transform=None, chunk_size=1000):\n",
    "        self.transform = transform\n",
    "        self.electron_path = electron_path\n",
    "        self.photon_path = photon_path\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # Get dataset sizes without loading full data\n",
    "        with h5py.File(electron_path, 'r') as f:\n",
    "            self.electron_len = len(f['X'])\n",
    "            \n",
    "        with h5py.File(photon_path, 'r') as f:\n",
    "            self.photon_len = len(f['X'])\n",
    "            \n",
    "        self.total_len = self.electron_len + self.photon_len\n",
    "        \n",
    "        # Create shuffled indices\n",
    "        self.indices = np.random.permutation(self.total_len)\n",
    "        \n",
    "        print(f\"Dataset initialized with {self.total_len} samples\")\n",
    "        print(f\"Electron samples: {self.electron_len}\")\n",
    "        print(f\"Photon samples: {self.photon_len}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Determine if the sample is from electron or photon dataset\n",
    "        if self.indices[idx] < self.electron_len:\n",
    "            with h5py.File(self.electron_path, 'r') as f:\n",
    "                sample = f['X'][self.indices[idx]]\n",
    "                label = 0  # Electron\n",
    "        else:\n",
    "            with h5py.File(self.photon_path, 'r') as f:\n",
    "                sample = f['X'][self.indices[idx] - self.electron_len]\n",
    "                label = 1  # Photon\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        sample = torch.from_numpy(sample).float()\n",
    "        label = torch.tensor(label).long()\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResidualBlock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet15\n",
    "class ResNet15(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet15, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        \n",
    "        # Average pooling and fully connected layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, device='cuda'):\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    patience = 10\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Track statistics\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_correct / val_total\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_epoch_loss)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            \n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (photon)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Electron', 'Photon'], \n",
    "                yticklabels=['Electron', 'Photon'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'true_labels': true_labels,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 3*num_samples))\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        data, label = dataset[idx]\n",
    "        \n",
    "        # Energy channel\n",
    "        im0 = axes[i, 0].imshow(data[0], cmap='viridis')\n",
    "        axes[i, 0].set_title(f\"Sample {idx} - {'Photon' if label == 1 else 'Electron'} - Energy\")\n",
    "        axes[i, 0].axis('off')\n",
    "        fig.colorbar(im0, ax=axes[i, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Time channel\n",
    "        im1 = axes[i, 1].imshow(data[1], cmap='plasma')\n",
    "        axes[i, 1].set_title(f\"Sample {idx} - {'Photon' if label == 1 else 'Electron'} - Time\")\n",
    "        axes[i, 1].axis('off')\n",
    "        fig.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, dataset, layer_name, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualize feature maps from a specific layer of the model for a sample image\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Select a random sample\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    input_img, label = dataset[idx]\n",
    "    input_img = input_img.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Register hook to get intermediate activations\n",
    "    activations = {}\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        activations[layer_name] = output.detach().cpu()\n",
    "    \n",
    "    # Register hook\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name:\n",
    "            hook = module.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_img)\n",
    "    \n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "    \n",
    "    # Get the feature maps\n",
    "    feature_maps = activations[layer_name][0]  # First sample in batch\n",
    "    num_features = min(16, feature_maps.size(0))  # Display at most 16 feature maps\n",
    "    \n",
    "    # Plot the feature maps\n",
    "    fig, axes = plt.subplots(int(np.ceil(num_features/4)), 4, figsize=(12, 3*int(np.ceil(num_features/4))))\n",
    "    fig.suptitle(f\"Feature maps from {layer_name} for {'Photon' if label == 1 else 'Electron'}\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        ax = axes[i//4, i%4] if num_features > 4 else axes[i]\n",
    "        im = ax.imshow(feature_maps[i], cmap='viridis')\n",
    "        ax.set_title(f\"Filter {i}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide any empty subplots\n",
    "    for i in range(num_features, int(np.ceil(num_features/4))*4):\n",
    "        if i < int(np.ceil(num_features/4))*4:\n",
    "            axes[i//4, i%4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = os.path.join('datasets', 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "    photon_path = os.path.join('datasets', 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Visualize some samples\n",
    "    visualize_samples(full_dataset)\n",
    "    \n",
    "    # Split dataset into train (80%) and test (20%) sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Further split train set into train and validation\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    print(f\"Test set size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Create model\n",
    "    model = ResNet15(num_classes=2).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=50,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate model on test set\n",
    "    test_results = evaluate_model(trained_model, test_loader, device)\n",
    "    \n",
    "    # Visualize feature maps\n",
    "    visualize_feature_maps(trained_model, test_dataset, 'layer3.1.conv2', device)\n",
    "    \n",
    "    # Save model weights\n",
    "    torch.save(trained_model.state_dict(), 'resnet15_electron_photon_classifier.pth')\n",
    "    print(\"Model saved to resnet15_electron_photon_classifier.pth\")\n",
    "    \n",
    "    # Save test results\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_labels': test_results['true_labels'],\n",
    "        'predictions': test_results['predictions'],\n",
    "        'probabilities': test_results['probabilities']\n",
    "    })\n",
    "    results_df.to_csv('test_results.csv', index=False)\n",
    "    print(\"Test results saved to test_results.csv\")\n",
    "    \n",
    "    return trained_model, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_experiment():\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = 'electron_data.h5'  # Update with actual path\n",
    "    photon_path = 'photon_data.h5'      # Update with actual path\n",
    "\n",
    "    # Basic dataset without augmentation\n",
    "    basic_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Define augmentation transforms\n",
    "    class RandomNoise(object):\n",
    "        def __init__(self, noise_level=0.05):\n",
    "            self.noise_level = noise_level\n",
    "        \n",
    "        def __call__(self, sample):\n",
    "            noise = np.random.normal(0, self.noise_level, sample.shape)\n",
    "            noisy_sample = sample + noise\n",
    "            return noisy_sample\n",
    "    \n",
    "    class RandomFlip(object):\n",
    "        def __call__(self, sample):\n",
    "            if np.random.random() > 0.5:\n",
    "                return np.flip(sample, axis=1)  # Flip horizontally\n",
    "            return sample\n",
    "    \n",
    "    class RandomRotate90(object):\n",
    "        def __call__(self, sample):\n",
    "            k = np.random.randint(0, 4)  # 0, 1, 2, or 3 times 90 degrees\n",
    "            return np.rot90(sample, k=k, axes=(1, 2))\n",
    "    \n",
    "    # Create augmented dataset\n",
    "    class AugmentedParticleDataset(ParticleDataset):\n",
    "        def __init__(self, electron_path, photon_path):\n",
    "            super().__init__(electron_path, photon_path)\n",
    "            self.transform = transforms.Compose([\n",
    "                RandomNoise(),\n",
    "                RandomFlip(),\n",
    "                RandomRotate90()\n",
    "            ])\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "            \n",
    "            sample = self.data[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # Apply data augmentation\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "            \n",
    "            # Convert to torch tensors\n",
    "            sample = torch.from_numpy(sample).float()\n",
    "            label = torch.tensor(label).long()\n",
    "            \n",
    "            return sample, label\n",
    "    \n",
    "    # Create augmented dataset\n",
    "    augmented_dataset = AugmentedParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Split datasets\n",
    "    train_size = int(0.8 * len(basic_dataset))\n",
    "    test_size = len(basic_dataset) - train_size\n",
    "    basic_train, basic_test = random_split(basic_dataset, [train_size, test_size])\n",
    "    augmented_train, augmented_test = random_split(augmented_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = 64\n",
    "    basic_train_loader = DataLoader(basic_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    basic_val_loader = DataLoader(basic_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    augmented_train_loader = DataLoader(augmented_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    augmented_val_loader = DataLoader(augmented_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Train model without augmentation\n",
    "    basic_model = ResNet15(num_classes=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(basic_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    basic_model, basic_history = train_model(\n",
    "        model=basic_model,\n",
    "        train_loader=basic_train_loader,\n",
    "        val_loader=basic_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=30,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train model with augmentation\n",
    "    augmented_model = ResNet15(num_classes=2).to(device)\n",
    "    optimizer = optim.Adam(augmented_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    augmented_model, augmented_history = train_model(\n",
    "        model=augmented_model,\n",
    "        train_loader=augmented_train_loader,\n",
    "        val_loader=augmented_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=30,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Compare results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(basic_history['val_loss'], label='Without Augmentation')\n",
    "    plt.plot(augmented_history['val_loss'], label='With Augmentation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Validation Loss Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(basic_history['val_acc'], label='Without Augmentation')\n",
    "    plt.plot(augmented_history['val_acc'], label='With Augmentation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Validation Accuracy Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Final validation accuracy without augmentation:\", basic_history['val_acc'][-1])\n",
    "    print(\"Final validation accuracy with augmentation:\", augmented_history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning():\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = 'electron_data.h5'  # Update with actual path\n",
    "    photon_path = 'photon_data.h5'      # Update with actual path\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Split dataset into train (80%) and test (20%) sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Further split train set into train and validation\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Define hyperparameter combinations\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    weight_decays = [1e-4, 1e-5, 1e-6]\n",
    "    dropout_rates = [0.2, 0.3, 0.4]\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Grid search\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decays:\n",
    "            for dr in dropout_rates:\n",
    "                print(f\"Training with lr={lr}, weight_decay={wd}, dropout={dr}\")\n",
    "                \n",
    "                # Create model with specified dropout rate\n",
    "                class ResNet15WithDropout(ResNet15):\n",
    "                    def __init__(self, num_classes=2, dropout_rate=0.3):\n",
    "                        super().__init__(num_classes)\n",
    "                        self.dropout = nn.Dropout(dropout_rate)\n",
    "                \n",
    "                model = ResNet15WithDropout(num_classes=2, dropout_rate=dr).to(device)\n",
    "                \n",
    "                # Define loss function and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "                \n",
    "                # Train model for fewer epochs for quicker tuning\n",
    "                trained_model, history = train_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    num_epochs=20,  # Reduced epochs for quicker tuning\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                trained_model.eval()\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = trained_model(inputs)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                val_accuracy = val_correct / val_total\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'weight_decay': wd,\n",
    "                    'dropout_rate': dr,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'final_val_loss': history['val_loss'][-1]\n",
    "                })\n",
    "    \n",
    "    # Convert results to DataFrame and find best hyperparameters\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_config = results_df.loc[results_df['val_accuracy'].idxmax()]\n",
    "    \n",
    "    print(\"\\nHyperparameter Tuning Results:\")\n",
    "    print(results_df.sort_values('val_accuracy', ascending=False))\n",
    "    print(\"\\nBest Configuration:\")\n",
    "    print(best_config)\n",
    "    \n",
    "    return best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_model():\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set paths to data\n",
    "    electron_path = os.path.join('datasets', 'electron.hdf5')\n",
    "    photon_path = os.path.join('datasets', 'photon.hdf5')    \n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = ParticleDataset(electron_path, photon_path)\n",
    "    \n",
    "    # Split dataset into train (80%) and test (20%) sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create test loader\n",
    "    batch_size = 64\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Create 3 models with different architectures\n",
    "    class ResNet15Wide(ResNet15):\n",
    "        def __init__(self, num_classes=2):\n",
    "            super(ResNet15, self).__init__()\n",
    "            self.in_channels = 64\n",
    "            \n",
    "            # Initial convolution - wider channels\n",
    "            self.conv1 = nn.Conv2d(2, 96, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(96)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            \n",
    "            # Residual blocks - wider channels\n",
    "            self.layer1 = self._make_layer(96, 2, stride=1)\n",
    "            self.layer2 = self._make_layer(192, 2, stride=2)\n",
    "            self.layer3 = self._make_layer(384, 2, stride=2)\n",
    "            \n",
    "            # Average pooling and fully connected layer\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(384, num_classes)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    class ResNet15Deep(ResNet15):\n",
    "        def __init__(self, num_classes=2):\n",
    "            super(ResNet15, self).__init__()\n",
    "            self.in_channels = 64\n",
    "            \n",
    "            # Initial convolution\n",
    "            self.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            \n",
    "            # Residual blocks - deeper architecture\n",
    "            self.layer1 = self._make_layer(64, 3, stride=1)  # One more block\n",
    "            self.layer2 = self._make_layer(128, 3, stride=2)  # One more block\n",
    "            self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "            \n",
    "            # Average pooling and fully connected layer\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(256, num_classes)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    # Load pre-trained models\n",
    "    model1 = ResNet15(num_classes=2).to(device)\n",
    "    model1.load_state_dict(torch.load('resnet15_model1.pth'))\n",
    "    \n",
    "    model2 = ResNet15Wide(num_classes=2).to(device)\n",
    "    model2.load_state_dict(torch.load('resnet15_wide_model2.pth'))\n",
    "    \n",
    "    model3 = ResNet15Deep(num_classes=2).to(device)\n",
    "    model3.load_state_dict(torch.load('resnet15_deep_model3.pth'))\n",
    "    \n",
    "    # Function to evaluate ensemble\n",
    "    def evaluate_ensemble(models, test_loader, device):\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        true_labels = []\n",
    "        ensemble_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(test_loader, desc=\"Evaluating Ensemble\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Get probabilities from each model\n",
    "                probs_list = []\n",
    "                for model in models:\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                    probs_list.append(probs)\n",
    "                \n",
    "                # Average the probabilities\n",
    "                ensemble_prob = torch.stack(probs_list).mean(dim=0)\n",
    "                \n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                ensemble_probs.extend(ensemble_prob[:, 1].cpu().numpy())  # Probability of class 1 (photon)\n",
    "        \n",
    "        # Convert probabilities to predictions\n",
    "        ensemble_preds = [1 if p > 0.5 else 0 for p in ensemble_probs]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_labels, ensemble_preds)\n",
    "        precision = precision_score(true_labels, ensemble_preds)\n",
    "        recall = recall_score(true_labels, ensemble_preds)\n",
    "        f1 = f1_score(true_labels, ensemble_preds)\n",
    "        auc = roc_auc_score(true_labels, ensemble_probs)\n",
    "        conf_matrix = confusion_matrix(true_labels, ensemble_preds)\n",
    "        \n",
    "        print(f\"Ensemble Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Ensemble Precision: {precision:.4f}\")\n",
    "        print(f\"Ensemble Recall: {recall:.4f}\")\n",
    "        print(f\"Ensemble F1 Score: {f1:.4f}\")\n",
    "        print(f\"Ensemble ROC AUC: {auc:.4f}\")\n",
    "        print(\"Ensemble Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    ensemble_models = [model1, model2, model3]\n",
    "    ensemble_results = evaluate_ensemble(ensemble_models, test_loader, device)\n",
    "    \n",
    "    return ensemble_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run main model training\n",
    "print(\"=== Training Main Model ===\")\n",
    "try:\n",
    "    model, test_results = main()\n",
    "    print(\"\\nMain model training completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in main model training: {e}\")\n",
    "    model, test_results = None, None\n",
    "\n",
    "# Only proceed with experiments if main training was successful\n",
    "if model is not None and test_results is not None:\n",
    "    print(\"\\n=== Running Additional Experiments ===\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n=== Data Augmentation Experiment ===\")\n",
    "        augmentation_experiment()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in augmentation experiment: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n=== Hyperparameter Tuning ===\")\n",
    "        best_hyperparams = hyperparameter_tuning()\n",
    "        print(\"\\nBest hyperparameters found:\", best_hyperparams)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in hyperparameter tuning: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n=== Model Ensemble ===\")\n",
    "        ensemble_results = create_ensemble_model()\n",
    "        print(\"\\nEnsemble evaluation completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ensemble evaluation: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping additional experiments due to main training failure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
